 # ============================================================================
# Codex CLI — Comprehensive Configuration Template
# Updated: 2025-10-31
# Location to use in practice: ~/.codex/config.toml (aka $CODEX_HOME/config.toml)
# ============================================================================

####################################
# Root (top-level) keys
####################################

model = "gpt-5"                                # default model
# model_provider = "openai"                      # key under [model_providers]
# model_context_window = 200000                   # tokens; override when unknown
# model_max_output_tokens = 100000                # tokens; override when unknown
#
# model_auto_compact_token_limit = 60000          # tokens; auto-compact history when exceeded
#
approval_policy = "never"                  # "untrusted" | "on-failure" | "on-request" | "never"
sandbox_mode = "danger-full-access"                      # "read-only" | "workspace-write" | "danger-full-access"
# disable_response_storage = false                # set true for ZDR accounts
#
# review_model = "o3"                          # Optional override for /review code review model
# developer_instructions = "/abs/path/dev.md"  # Injected developer-role instructions (string or file path)
# compact_prompt = "/abs/path/compact.md"      # Optional compact prompt override (string or file path)
# log_user_prompt = true                       # Persist plaintext prompts in history (default: false)
#
# file_opener = "vscode"                          # "vscode" | "vscode-insiders" | "windsurf" | "cursor" | "none"
# hide_agent_reasoning = false
show_raw_agent_reasoning = true
#
# Disable multi-line paste burst submissions in TUI composer
disable_paste_burst = false
# Experimental: render reasoning summaries in TUI (if supported)
use_experimental_reasoning_summary = false
model_reasoning_effort = "high"               # "minimal" | "low" | "medium" | "high" | "none"
# model_reasoning_summary = "auto"                # "auto" | "concise" | "detailed" | "none"
# model_reasoning_summary_format = "auto"         # Optional format hint for reasoning (leave "auto")
# model_verbosity = "medium"                      # GPT‑5 family only; "low" | "medium" | "high"
# model_supports_reasoning_summaries = false      # force-enable reasoning block
#
# chatgpt_base_url = "https://chatgpt.com/backend-api/"     # advanced
# experimental_resume = "/abs/path/resume.jsonl"            # advanced
# experimental_instructions_file = "/abs/path/base.txt"     # advanced
# experimental_use_exec_command_tool = false                 # advanced
# responses_originator_header_internal_override = "codex_cli_rs"  # internal/testing
#
# project_doc_max_bytes = 32768                    # bytes to read from AGENTS.md
# project_doc_fallback_filenames = ["PROJECT_GUIDE.md", "PROJECT_RULES.md"]
# preferred_auth_method = "chatgpt"                # or "apikey"
#
# forced_chatgpt_workspace_id = "workspace-123"    # Restrict login to a single ChatGPT workspace
# forced_login_method = "chatgpt"                  # "chatgpt" | "api"; reject other login flows
# cli_auth_credentials_store = "/abs/path/auth.json"       # Override path for CLI auth store
# mcp_oauth_credentials_store = "/abs/path/mcp_auth.json"  # Override path for MCP OAuth store
#
# profile = "default"                              # active [profiles] entry
#
# instructions = "You are a helpful assistant."    # extra system instructions (merged with AGENTS.md)
#
####################################
# Features (tool toggles)
####################################

[features]
# web_search_request = true                 # Native Responses web search (same as --search)
# view_image_tool = true                    # Attach local images
# unified_exec = false                      # PTY-based exec runtime (experimental)
# streamable_shell = false                  # Stream shell output (requires unified_exec)
# rmcp_client = false                       # Enable MCP client support managed by the CLI
# apply_patch_freeform = false              # Allow freeform apply_patch payloads
# experimental_sandbox_command_assessment = false  # Risk assessment before shell execution
# ghost_commit = false                      # Capture ghost snapshots for undo/restore
# enable_experimental_windows_sandbox = false      # Windows-only sandbox integration

####################################
# Shell environment policy for spawned processes
####################################

[shell_environment_policy]
inherit = "all"                  # "all" | "core" | "none"
# ignore_default_excludes = false   # when false, drops vars whose NAMES contain KEY/SECRET/TOKEN
# exclude = ["AWS_*", "AZURE_*"]   # case-insensitive globs
# set = { CI = "1" }                # force-set values
# include_only = ["PATH", "HOME"]   # keep-only whitelist (globs)
# experimental_use_profile = false   # advanced

####################################
# Sandbox settings (apply when sandbox_mode = "workspace-write")
####################################

[sandbox_workspace_write]
# writable_roots = ["/additional/writable/path"]
network_access = true
# exclude_tmpdir_env_var = false
# exclude_slash_tmp = false

####################################
# History persistence
####################################

[history]
persistence = "save-all"          # "save-all" | "none"
# max_bytes = 10485760               # not strictly enforced yet

####################################
# MCP servers (tools via stdio)
####################################

# [mcp_servers.example]
# command = "npx"
# args = ["-y", "mcp-server-example"]
# env = { API_KEY = "value" }

####################################
# Model providers (extend/override built-ins)
####################################

# Built-in OpenAI provider can be customized here.
# [model_providers.openai]
# name = "OpenAI"
# base_url = "https://api.openai.com/v1"          # OPENAI_BASE_URL env var also supported
# wire_api = "responses"                           # "responses" | "chat"
# query_params = {}
# http_headers = { version = "0.0.0" }
# env_http_headers = { "OpenAI-Organization" = "OPENAI_ORGANIZATION", "OpenAI-Project" = "OPENAI_PROJECT" }
# request_max_retries = 4
# stream_max_retries = 5
# stream_idle_timeout_ms = 300000
# requires_openai_auth = true

# Chat Completions-compatible provider example
# [model_providers.openai-chat-completions]
# name = "OpenAI using Chat Completions"
# base_url = "https://api.openai.com/v1"
# env_key = "OPENAI_API_KEY"
# wire_api = "chat"
# query_params = {}

# Azure example (requires api-version)
# [model_providers.azure]
# name = "Azure"
# base_url = "https://YOUR_PROJECT_NAME.openai.azure.com/openai"
# env_key = "AZURE_OPENAI_API_KEY"
# query_params = { api-version = "2025-04-01-preview" }

# Local OSS (Ollama) example
# [model_providers.ollama]
# name = "Ollama"
# base_url = "http://localhost:11434/v1"
# wire_api = "chat"

####################################
# Profiles (bundled overrides you can switch with --profile)
####################################

# [profiles.default]
# model = "gpt-5"
# model_provider = "openai"
# approval_policy = "never"
# disable_response_storage = false
# model_reasoning_effort = "high"
# model_reasoning_summary = "auto"
# model_verbosity = "medium"
# chatgpt_base_url = "https://chatgpt.com/backend-api/"
# experimental_instructions_file = "/abs/path/base_instructions.txt"

# [profiles.zdr]
# model = "o3"
# model_provider = "openai"
# approval_policy = "on-failure"
# disable_response_storage = true

####################################
# Projects trust (per-absolute-path)
####################################

# [projects."/absolute/path/to/your/repo"]
# trust_level = "trusted"    # mark as trusted

####################################
# TUI preferences
####################################

# [tui]
# hide_full_access_warning = false          # Suppress danger-full-access banner
# windows_wsl_setup_acknowledged = false    # Windows-only: suppress WSL onboarding tip

####################################
# Quick reference: CLI overrides (for sharing/docs)
####################################
# codex -c model="o3"
# codex -c 'features.web_search_request=true' "your prompt here"
# codex --sandbox workspace-write --ask-for-approval on-request

# Notes:
# - The TUI flag --search maps to [features].web_search_request in TOML.
# - The built-in OpenAI provider also respects env: OPENAI_BASE_URL, OPENAI_ORGANIZATION, OPENAI_PROJECT.
# - OSS provider can be influenced by env: CODEX_OSS_BASE_URL, CODEX_OSS_PORT (envs, not TOML).

# NOTE: network_access is only valid under [sandbox_workspace_write].
# When sandbox_mode="danger-full-access", network is already unrestricted.
